В данном репозитории содержатся работы Дудко Александра по предмету "Системы и методы принятия решений".

# <center><b>Метрические алгоритмы</b></center>

**Гипотеза компактности** - схожим объекам соответствуют схожие ответы.
Для формализации понятия **сходства** вводится функция расстояния в пространстве X.

**Метрические методы обучения** - методы, основанные на анализе сходства объектов.
Метрические алгоритмы относят к алгоритмам **ленивого** обучения.

**1. Алгоритм классификации "1NN":**
   
Классифицируемый объект относим к тому классу, к которому принадлежит ближайший по заданной метрике "сосед" из выборки:

![w](https://latex.codecogs.com/gif.latex?w%28i%2C%20u%29%20%3D%20%5Bi%20%3D%201%5D%3B)

В реализованном методе выбрана Евклидова метрика.

В качестве выборки был взят набор "Ирисы Фишера"

Карта классификации выглядит следующим образом:

![1NN](https://github.com/SimfikDuke/Rprojects/blob/master/img/1NN_classification.png)
Недостатки:

- Неустойчивость к погрешностям. Если среди обучающих объектов есть выброс — объект, находящийся в окружении объектов чужого класса, то не только он сам будет классифицирован неверно, но и т.е. окружающие его объекты, для которых он окажется ближайшим.
- Отсутствие параметров, которые можно было бы настраивать по выборке. Алгоритм полностью зависит от того, насколько удачно выбрана метрика ρ.
- В результате — низкое качество классификации.

**2. Алгоритм классификации "KNN" k-ближайших соседей:**

Все объекты выборки сортируются по удаленности от классифицируемого объекта. Выбираются k ближайших соседей.
Классифицируемый объект относим к тому классу, экземпляров которого больше в наборе из полученных k соседей:
![w](https://latex.codecogs.com/gif.latex?w%28i%2C%20u%29%20%3D%20%5Bi%20%5Cleq%20k%5D%3B)

В реализованном методе выбрана Евклидова метрика.

В качестве выборки был взят набор "Ирисы Фишера".

Оценка скользящего контроля LOO (leave-one-out) алгоритма k-ближайших соседей для данного набора показала, что классификация более точна при k=6.

График оценки скользящего контроля, а также карта классификации выглядят следующим образом:
![KNN](https://github.com/SimfikDuke/Rprojects/blob/master/img/LOO_KNN.png)

**3. LOO (leave-one-out) для алгоритма KNN:**

Оценка скользящего контроля для различных значений ![k](http://www.sciweavers.org/upload/Tex2Img_1538633578/render.png) алгоритма KNN: 

![LOO(K)](https://github.com/SimfikDuke/Rprojects/blob/master/img/LOO(K).png)

**4. Алгоритм классификации "KWNN" k-взвешенных соседей:**
   
Все объекты выборки сортируются по удаленности от классифицируемого объекта. Выбираются ![k](http://www.sciweavers.org/upload/Tex2Img_1538633578/render.png) ближайших соседей.
Классифицируемый объект относим к тому классу, суммарный вес которого больше.
![menshe](http://www.sciweavers.org/upload/Tex2Img_1538633992/render.png)
В реализованном методе выбрана Евклидова метрика.
В качестве выборки был взят набор "Ирисы Фишера".
Для составления карты классификации параметр ![k](http://www.sciweavers.org/upload/Tex2Img_1538633578/render.png) равен 6, а ![www](http://www.sciweavers.org/upload/Tex2Img_1538633869/render.png).
Карта классификации выглядит следующим образом:

![KNN](https://github.com/SimfikDuke/Rprojects/blob/master/img/KNN_classification.png)

**5. LOO (leave-one-out) для алгоритма KWNN:**

Оценка скользящего контроля для различных значений $w$ алгоритма KWNN при ![w](http://www.sciweavers.org/upload/Tex2Img_1538633788/render.png): 

![LOO(K,W)](https://github.com/SimfikDuke/Rprojects/blob/master/img/LOO(K,W)_KWNN.png)

На данном графике можно наблюдать превосходсво алгоритма KWNN над, поскольку если в алгоритм KWNN поставить ![ww](http://www.sciweavers.org/upload/Tex2Img_1538633831/render.png), то полученный алгоритм будет подобен алгоритму KNN. 
На графике LOO(K,W) можно наблюдать, что ошибка при ![www](http://www.sciweavers.org/upload/Tex2Img_1538633869/render.png) меньше, чем при ![ww](http://www.sciweavers.org/upload/Tex2Img_1538633831/render.png)
